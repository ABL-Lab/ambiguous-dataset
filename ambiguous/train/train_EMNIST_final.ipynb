{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import dependencies\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchvision\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import h5py\n",
    "from copy import deepcopy\n",
    "import ambiguous.models.cvae\n",
    "from ambiguous.models.cvae import *\n",
    "from ambiguous.dataset.dataset import partition_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CVAE = False\n",
    "LOCAL_CKPT = False\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "print(device)\n",
    "root='/home/mila/n/nizar.islah/expectation-clamp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x.rot90(1,[2,1]).flip(2)\n",
    "])\n",
    "dataset = datasets.EMNIST(root=root, download=False, split='byclass', train=True, transform=transform)\n",
    "new_dataset = partition_dataset(dataset, range(10, 36))\n",
    "train_set, val_set = torch.utils.data.random_split(new_dataset, [round(0.8*len(new_dataset)), round(0.2*len(new_dataset))])\n",
    "test_set = datasets.EMNIST(root=root, download=False, split='byclass', train=False, transform=transform)\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'f', 'm', 'x', 'm', 'c', 'c', 'e', 'a', 't', 'r', 'p', 'k', 'c', 'o', 'a', 'c', 'i', 'a', 'n', 'r', 'n', 'o', 'w', 'i', 't', 'o', 'c', 's', 'd', 'o', 'c', 'b', 'p', 'z', 'o', 'f', 'i', 'i', 'p', 'u', 'e', 'h', 's', 'x', 'm', 'z', 'a', 'o', 'd', 'm', 'w', 'h', 'u', 'o', 'd', 's', 't', 'h', 'w', 'y', 'r', 's', 's']\n"
     ]
    }
   ],
   "source": [
    "x,t=next(iter(train_loader))\n",
    "torchvision.utils.save_image(x, 'image.pdf', nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMNIST_CVAE(\n",
       "  (encoder): EMNIST_Encoder(\n",
       "    (MLP): Sequential(\n",
       "      (L0): Linear(in_features=810, out_features=1024, bias=True)\n",
       "      (A0): ReLU()\n",
       "      (L1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (A1): ReLU()\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=512, out_features=20, bias=True)\n",
       "    (fc_logvar): Linear(in_features=512, out_features=20, bias=True)\n",
       "  )\n",
       "  (decoder): EMNIST_Decoder(\n",
       "    (MLP): Sequential(\n",
       "      (L0): Linear(in_features=46, out_features=512, bias=True)\n",
       "      (A0): ReLU()\n",
       "      (L1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (A1): ReLU()\n",
       "      (L2): Linear(in_features=1024, out_features=784, bias=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_layers = [28*28, 1024, 512]\n",
    "dec_layers = [512, 1024, 28*28]\n",
    "latent_dim = 20\n",
    "if TRAIN_CVAE:\n",
    "    wandb.init(project=\"EMNIST_CVAE\", entity=\"team-nizar\")\n",
    "    wandb_logger = WandbLogger(project=\"EMNIST_CVAE\", log_model=\"all\")\n",
    "    model = EMNIST_CVAE(latent_dim, enc_layers, dec_layers, n_classes=26, conditional=True).to(device)\n",
    "    trainer = pl.Trainer(gpus=1,logger=wandb_logger,max_epochs=100,auto_lr_find=True)\n",
    "    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
    "    torch.save(model.state_dict(), \"emnist_cvae.pth\")\n",
    "    \n",
    "else:\n",
    "    if LOCAL_CKPT:\n",
    "        ckpt_path = \"\"\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    else:\n",
    "        run = wandb.init()\n",
    "        artifact = run.use_artifact('team-nizar/EMNIST_CVAE/model-pkia9rhl:v29', type='model')\n",
    "        artifact_dir = artifact.download()\n",
    "        ckpt = torch.load(artifact_dir+'/model.ckpt')\n",
    "        model = EMNIST_CVAE(latent_dim, enc_layers, dec_layers, n_classes=26, conditional=True).to(device)\n",
    "        model.load_state_dict(ckpt['state_dict'])\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_imgs(model, n_samples, n_latent, n_classes=26):\n",
    "    targets = torch.randint(0, n_classes, (n_samples,))\n",
    "    c = torch.zeros(n_samples, n_classes).to(device)\n",
    "    c[range(n_samples), targets] = 1\n",
    "    z = torch.randn(n_samples, n_latent).to(device)\n",
    "    rec = model.decoder(z, c).view(-1, 1, 28, 28)\n",
    "    torchvision.utils.save_image(rec, \"reconstruction.pdf\", nrow=8)\n",
    "    return\n",
    "\n",
    "gen_imgs(model, 64, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56b94a22e0b30145bc461d50c8481827ccfe484109677afaff2bcdc486ba3cbc"
  },
  "kernelspec": {
   "display_name": "Python 3 (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "56b94a22e0b30145bc461d50c8481827ccfe484109677afaff2bcdc486ba3cbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
