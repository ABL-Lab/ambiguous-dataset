{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import dependencies\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchvision\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import h5py\n",
    "from copy import deepcopy\n",
    "import ambiguous.models.cvae\n",
    "from ambiguous.models.cvae import *\n",
    "from ambiguous.dataset.dataset import partition_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CVAE = True\n",
    "LOCAL_CKPT = False\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "print(device)\n",
    "root='/home/mila/n/nizar.islah/expectation-clamp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = datasets.MNIST(root=root, download=True, train=True, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [round(0.8*len(dataset)), round(0.2*len(dataset))])\n",
    "test_set = datasets.MNIST(root=root, download=True, train=False, transform=transform)\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 159783.01it/s]\n"
     ]
    }
   ],
   "source": [
    "s = range(100)\n",
    "t = tqdm(total=len(s))\n",
    "for x in s:\n",
    "    t.update()\n",
    "t.refresh()  # force print final state\n",
    "\n",
    "t.reset()  # reuse bar\n",
    "for x in s:\n",
    "    t.update()\n",
    "t.close()  # close the bar permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,t=next(iter(train_loader))\n",
    "torchvision.utils.save_image(x, 'image.png', nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:43,  6.19s/it][00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [00:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/train/train_MNIST_final_conv.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmila/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/train/train_MNIST_final_conv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m y_ \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mrand(images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m n_cls)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mLongTensor)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmila/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/train/train_MNIST_final_conv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m y_label_ \u001b[39m=\u001b[39m onehot[y_]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmila/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/train/train_MNIST_final_conv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m rec, mu, logvar \u001b[39m=\u001b[39m model((images, labels_fill_, y_label_))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmila/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/train/train_MNIST_final_conv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m loss_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_function(rec, images, mu, logvar)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmila/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/train/train_MNIST_final_conv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_dict[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m/\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ambiguous-dataset/ambiguous/models/cvae.py:299\u001b[0m, in \u001b[0;36mConv_CVAE.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    298\u001b[0m     x, y_enc, y_dec \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 299\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode((x,y_enc))\n\u001b[1;32m    300\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m  [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode((z,y_dec)), mu, log_var]\n",
      "File \u001b[0;32m~/ambiguous-dataset/ambiguous/models/cvae.py:261\u001b[0m, in \u001b[0;36mConv_CVAE.encode\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    Encodes the input by passing through the encoder network\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m    and returns the latent codes.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    :param input: (Tensor) Input tensor to encoder [N x C x H x W]\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m    :return: (Tensor) List of latent codes\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    262\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(result, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m     \u001b[39m# Split the result into mu and var components\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[39m# of the latent Gaussian distribution\u001b[39;00m\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ambiguous-dataset/ambiguous/models/cvae.py:153\u001b[0m, in \u001b[0;36mCatConv.forward\u001b[0;34m(self, xy)\u001b[0m\n\u001b[1;32m    151\u001b[0m x, y \u001b[39m=\u001b[39m xy\n\u001b[1;32m    152\u001b[0m out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m--> 153\u001b[0m out2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(y)\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([out1, out2], \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_cls=10\n",
    "img_size=28\n",
    "lr=1e-3\n",
    "num_epochs=50\n",
    "latent_dim = 10\n",
    "\n",
    "onehot = torch.zeros(n_cls, n_cls).to(device)\n",
    "onehot = onehot.scatter_(1, torch.LongTensor(range(n_cls)).view(n_cls,1).to(device), 1).view(n_cls, n_cls, 1, 1)\n",
    "fill = torch.zeros([n_cls, n_cls, img_size, img_size]).to(device)\n",
    "for i in range(n_cls):\n",
    "    fill[i, i, :, :] = 1\n",
    "\n",
    "if TRAIN_CVAE:\n",
    "    model = Conv_CVAE(\n",
    "    latent_dim = latent_dim,\n",
    "        n_cls=n_cls\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    n_train = len(train_loader)\n",
    "    n_val = len(val_loader)\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        running_loss = 0\n",
    "        for idx, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels_fill_ = fill[labels]\n",
    "            y_ = (torch.rand(images.size(0), 1) * n_cls).type(torch.LongTensor).squeeze().to(device)\n",
    "            y_label_ = onehot[y_]\n",
    "            rec, mu, logvar = model((images, labels_fill_, y_label_))\n",
    "            loss_dict = model.loss_function(rec, images, mu, logvar)\n",
    "            running_loss += loss_dict['loss'].item()/batch_size\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict['loss'].backward()\n",
    "            optimizer.step()\n",
    "            # wandb.log(loss_dict)\n",
    "        val_loss = 0\n",
    "        for _, (images, labels) in tqdm(enumerate(val_loader)):\n",
    "            with torch.no_grad():\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels_fill_ = fill[labels]\n",
    "                y_ = (torch.rand(images.size(0), 1) * n_cls).type(torch.LongTensor).squeeze()\n",
    "                y_label_ = onehot[y_]\n",
    "                rec, mu, logvar = model((images, labels_fill_, y_label_))\n",
    "                torchvision.utils.save_image(rec, \"reconstruction_valid.pdf\")\n",
    "                loss_dict = model.loss_function(rec, images, mu, logvar)\n",
    "                val_loss += loss_dict['loss'].item()/batch_size\n",
    "                loss_dict = {'loss_val':loss_dict['loss'], 'kld_val':loss_dict['kld'], 'rec_val':loss_dict['rec']}\n",
    "                # wandb.log(loss_dict)\n",
    "        torch.save(model, 'conv_cvae_mnist.pth')\n",
    "        print(f\"Epoch: {i+1} \\t Train Loss: {running_loss:.2f} \\t Val Loss: {val_loss:.2f}\")\n",
    "    \n",
    "else:\n",
    "    if LOCAL_CKPT:\n",
    "        ckpt_path = \"\"\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    else:\n",
    "        # run = wandb.init()\n",
    "        # artifact = run.use_artifact('team-nizar/EMNIST_CVAE/model-pkia9rhl:v29', type='model')\n",
    "        # artifact_dir = artifact.download()\n",
    "        ckpt_path = \"\"\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "        model = EMNIST_CVAE(latent_dim, enc_layers, dec_layers, n_classes=26, conditional=True).to(device)\n",
    "        model.load_state_dict(ckpt['state_dict'])\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]],\n",
       "\n",
       "         [[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[1.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for _, (images, labels) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_fill_ = fill[labels]\n",
    "        y_ = (torch.rand(images.size(0), 1) * n_cls).type(torch.LongTensor).squeeze()\n",
    "        y_label_ = onehot[y_]\n",
    "        rec, mu, logvar = model((images, labels_fill_, y_label_))\n",
    "    break\n",
    "\n",
    "torchvision.utils.save_image(rec, \"example.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/cuda/memory.py:114\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    701\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    703\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    704\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    705\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    706\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 707\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m callable(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    412\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _Formatter(\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:296\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    294\u001b[0m     end \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;28mself\u001b[39m[i]\n\u001b[1;32m    295\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))])\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:296\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    293\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    294\u001b[0m     end \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;28mself\u001b[39m[i]\n\u001b[1;32m    295\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))])\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:296\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    294\u001b[0m     end \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;28mself\u001b[39m[i]\n\u001b[1;32m    295\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))])\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:296\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    293\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    294\u001b[0m     end \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;28mself\u001b[39m[i]\n\u001b[1;32m    295\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))])\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/_tensor_str.py:298\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "y_label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m         labels_fill_ \u001b[38;5;241m=\u001b[39m fill[labels]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "for _, (images, labels) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_fill_ = fill[labels]\n",
    "        y_ = (torch.rand(images.size(0), 2) * n_cls).type(torch.LongTensor).squeeze()\n",
    "        y_label_ = onehot[y_]\n",
    "        rec, mu, logvar = model((images, labels_fill_, y_label_))\n",
    "    break\n",
    "    \n",
    "torchvision.utils.save_image(rec, \"example_ambi.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56b94a22e0b30145bc461d50c8481827ccfe484109677afaff2bcdc486ba3cbc"
  },
  "kernelspec": {
   "display_name": "Python 3 (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "56b94a22e0b30145bc461d50c8481827ccfe484109677afaff2bcdc486ba3cbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
