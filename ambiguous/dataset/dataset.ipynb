{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ambiguous.dataset.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_file(dataset_name='EMNIST',\n",
    "                     og_root='/share/datasets/',\n",
    "                     new_root='/home/nislah/ambiguous-dataset/ambiguous/data/EMNIST/',\n",
    "                     blend=0.5) # can specify pairs as argument, otherwise uses default from dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DatasetFromNPY('/home/nislah/ambiguous-dataset/ambiguous/data/EMNIST', train=True)\n",
    "testset = DatasetFromNPY('/home/nislah/ambiguous-dataset/ambiguous/data/EMNIST', train=False)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "print(len(trainset), len(testset))\n",
    "x, t = next(iter(trainloader))\n",
    "i = 0\n",
    "x.shape, [(t[0][i], t[1][i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(make_grid(x, nrow=8).detach().cpu().permute(1,2,0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dataset_to_file('MNIST', '/share/datasets/', '/home/nislah/ambiguous-dataset/ambiguous/data/MNIST/',blend=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DatasetFromNPY('/home/nislah/ambiguous-dataset/ambiguous/data/MNIST/', train=True)\n",
    "testset = DatasetFromNPY('/home/nislah/ambiguous-dataset/ambiguous/data/MNIST/', train=False)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "print(len(trainset), len(testset))\n",
    "x, t = next(iter(trainloader))\n",
    "i = 0\n",
    "x.shape, t[0][i], t[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(make_grid(x, nrow=8).detach().cpu().permute(1,2,0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new EMNIST (Conv conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import dependencies\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchvision\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import h5py\n",
    "from copy import deepcopy\n",
    "import ambiguous.models.cvae\n",
    "from ambiguous.models.cvae import *\n",
    "from ambiguous.dataset.dataset import partition_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "print(device)\n",
    "root='/home/mila/n/nizar.islah/expectation-clamp/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x.rot90(1,[2,1]).flip(2)\n",
    "])\n",
    "dataset = datasets.EMNIST(root=root, download=False, split='byclass', train=True, transform=transform)\n",
    "new_dataset = partition_dataset(dataset, range(10, 36))\n",
    "train_set, val_set = torch.utils.data.random_split(new_dataset, [round(0.8*len(new_dataset)), round(0.2*len(new_dataset))])\n",
    "test_set = datasets.EMNIST(root=root, download=False, split='byclass', train=False, transform=transform)\n",
    "# Dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../train/ConvCVAE.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls=26\n",
    "img_size=28\n",
    "lr=1e-3\n",
    "num_epochs=50\n",
    "latent_dim = 20\n",
    "\n",
    "onehot = torch.zeros(n_cls, n_cls).to(device)\n",
    "onehot = onehot.scatter_(1, torch.LongTensor(range(n_cls)).view(n_cls,1).to(device), 1).view(n_cls, n_cls, 1, 1)\n",
    "fill = torch.zeros([n_cls, n_cls, img_size, img_size]).to(device)\n",
    "for i in range(n_cls):\n",
    "    fill[i, i, :, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.5000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for _, (images, labels) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_fill_ = fill[labels]\n",
    "        y_ = (torch.rand(images.size(0), 1) * n_cls).type(torch.LongTensor).squeeze()\n",
    "        y_label_ = onehot[y_]\n",
    "        y2_ = (torch.rand(images.size(0), 1) * n_cls).type(torch.LongTensor).squeeze()\n",
    "        y2_label_ = onehot[y2_]\n",
    "        y_label = (y_label_ + y2_label_)/2\n",
    "        print(y_label)\n",
    "        rec, mu, logvar = model((images, labels_fill_, y_label))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(rec, \"example_mix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000]],\n",
       "\n",
       "        [[0.5000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.5000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]],\n",
       "\n",
       "        [[0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mila/n/nizar.islah/ambiguous-dataset/ambiguous/dataset'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m readout \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/mila/n/nizar.islah/expectation-clamp/results/conv_vae_20220504_133037/readout.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/serialization.py:1039\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "readout = torch.load('/home/mila/n/nizar.islah/expectation-clamp/results/conv_vae_20220504_133037/readout.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vae \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/mila/n/nizar.islah/expectation-clamp/results/conv_vae_20220504_124933/model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/test/lib/python3.8/site-packages/torch/serialization.py:1039\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "vae = torch.load('/home/mila/n/nizar.islah/expectation-clamp/results/conv_vae_20220504_124933/model.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
